name: Tests
on:
  push:
    branches:
      - main
    tags:
      - "*"
  pull_request:
  schedule:
    # Runs "At 00:01" (see https://crontab.guru)
    - cron: "1 0 * * *"

concurrency:
  # Include `github.event_name` to avoid pushes to `main` and
  # scheduled jobs canceling one another
  group: tests-${{ github.event_name }}-${{ github.ref }}
  cancel-in-progress: true

defaults:
  # Required shell entrypoint to have properly activated conda environments
  run:
    shell: bash -l {0}

jobs:
  runtime:
    name: Runtime - ${{ matrix.os }}, Python ${{ matrix.python-version }}, Runtime ${{ matrix.runtime-version }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python-version: ["3.9"]
        runtime-version: ["upstream", "latest", "0.0.4", "0.1.0"]

    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0

      - name: Set up environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          condarc-file: ci/condarc
          python-version: ${{ matrix.python-version }}
          environment-file: ci/environment.yml

      - name: Install coiled-runtime
        env:
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
        run: |
          python ci/create_runtime_meta.py
          source ci/scripts/install_coiled_runtime.sh

      - name: Run Coiled Runtime Tests
        id: test
        env:
          DASK_COILED__TOKEN: ${{ secrets.COILED_BENCHMARK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.RUNTIME_CI_BOT_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RUNTIME_CI_BOT_AWS_SECRET_ACCESS_KEY }}
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
          DB_NAME: runtime-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db
          BENCHMARK: true
        run: bash ci/scripts/run_tests.sh tests/runtime

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: runtime-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}
          path: runtime-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db

  benchmarks:
    name: Benchmarks - ${{ matrix.os }}, Python ${{ matrix.python-version }}, Runtime ${{ matrix.runtime-version }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python-version: ["3.9"]
        runtime-version: ["upstream", "latest", "0.0.4", "0.1.0"]

    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0

      - name: Set up environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          condarc-file: ci/condarc
          python-version: ${{ matrix.python-version }}
          environment-file: ci/environment.yml

      - name: Install coiled-runtime
        env:
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
        run: |
          python ci/create_runtime_meta.py
          source ci/scripts/install_coiled_runtime.sh

      - name: Run benchmarking tests
        id: benchmarking_tests
        env:
          DASK_COILED__TOKEN: ${{ secrets.COILED_BENCHMARK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.RUNTIME_CI_BOT_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RUNTIME_CI_BOT_AWS_SECRET_ACCESS_KEY }}
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
          DB_NAME: benchmark-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db
          BENCHMARK: true
        run: bash ci/scripts/run_tests.sh tests/benchmarks

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}
          path: benchmark-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db

  stability:
    name: Stability - ${{ matrix.os }}, Python ${{ matrix.python-version }}, Runtime ${{ matrix.runtime-version }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python-version: ["3.8", "3.9", "3.10"]
        runtime-version: ["upstream", "latest", "0.0.4", "0.1.0"]
        include:
          - python-version: "3.9"
            runtime-version: "latest"
            os: "windows-latest"
          - python-version: "3.9"
            runtime-version: "latest"
            os: "macos-latest"

    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0

      - name: Set up environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          condarc-file: ci/condarc
          python-version: ${{ matrix.python-version }}
          environment-file: ci/environment.yml

      - name: Install coiled-runtime
        env:
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
        run: |
          python ci/create_runtime_meta.py
          source ci/scripts/install_coiled_runtime.sh

      - name: Run stability tests
        id: stability_tests
        env:
          DASK_COILED__TOKEN: ${{ secrets.COILED_BENCHMARK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.RUNTIME_CI_BOT_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RUNTIME_CI_BOT_AWS_SECRET_ACCESS_KEY }}
          COILED_RUNTIME_VERSION: ${{ matrix.runtime-version }}
          DB_NAME: stability-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db
          BENCHMARK: true
          CLUSTER_DUMP: true
        run: bash ci/scripts/run_tests.sh tests/stability

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: stability-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}
          path: stability-${{ matrix.os }}-${{ matrix.runtime-version }}-py${{ matrix.python-version }}.db

  process-results:
    needs: [runtime, benchmarks, stability]
    name: Combine separate benchmark results
    if: always() && github.repository == 'coiled/coiled-runtime'
    runs-on: ubuntu-latest
    concurrency:
      # Fairly strict concurrency rule to avoid stepping on benchmark db.
      # Could eventually replace with a real db in coiled, RDS, or litestream
      group: process-benchmarks
      cancel-in-progress: false
    steps:
      - uses: actions/checkout@v2

      - uses: actions/setup-python@v4

      - name: Install dependencies
        run: pip install alembic

      - uses: actions/download-artifact@v3
        with:
          path: benchmarks

      - name: Download benchmark db
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RUNTIME_CI_BOT_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RUNTIME_CI_BOT_AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
          DB_NAME: benchmark.db
        run: |
          aws s3 cp s3://coiled-runtime-ci/benchmarks/$DB_NAME . || true

      - name: Combine benchmarks
        run: |
          ls -lhR benchmarks
          bash ci/scripts/combine-dbs.sh

      - name: Upload benchmark db
        if: always() && github.ref == 'refs/heads/main' && github.repository == 'coiled/coiled-runtime'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RUNTIME_CI_BOT_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RUNTIME_CI_BOT_AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
          DB_NAME: benchmark.db
        run: |
          aws s3 cp $DB_NAME s3://coiled-runtime-ci/benchmarks/

      - name: Upload benchmark results as artifact
        uses: actions/upload-artifact@v3
        with:
          name: benchmark
          path: benchmark.db

  regressions:
    needs: process-results
    # Always check for regressions, as this can be skipped even if an indirect dependency fails (like a test run)
    if: always()
    name: Detect regressions
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0

      - uses: actions/download-artifact@v3
        with:
          name: benchmark

      - name: Set up environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: "3.9"
          environment-file: ci/environment-dashboard.yml

      - name: Run detect regressions
        run: |
          python detect_regressions.py
      
      - name: Create regressions summary
        if: always()
        run: |
          echo "$(<regressions_summary.md)" >> $GITHUB_STEP_SUMMARY

  report:
    name: report
    needs: [runtime, benchmarks, stability, regressions]
    if: |
      always()
      && github.event_name != 'pull_request'
      && github.repository == 'coiled/coiled-runtime'
      && (needs.runtime.result == 'failure' ||
          needs.benchmarks.result == 'failure' ||
          needs.stability.result == 'failure' ||
          needs.regressions.result == 'failure') 

    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    steps:
      - uses: actions/checkout@v2
      - name: Report failures
        uses: actions/github-script@v3
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const workflow_url = `https://github.com/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}`
            const issue_body = `[Workflow Run URL](${workflow_url})`
            github.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: issue_body,
                title: "⚠️ CI failed ⚠️",
                labels: ["ci-failure"],
            })


  static-site:
    needs: process-results
    # Always generate the site, as this can be skipped even if an indirect dependency fails (like a test run)
    if: always()
    name: Build static dashboards
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0

      - uses: actions/download-artifact@v3
        with:
          name: benchmark

      - name: Set up environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-variant: Mambaforge
          use-mamba: true
          python-version: "3.9"
          environment-file: ci/environment-dashboard.yml

      - name: Generate dashboards
        run: |
          python dashboard.py

      - name: Upload benchmark results as artifact
        uses: actions/upload-artifact@v3
        with:
          name: static-site
          path: static/

      - name: Deploy 🚀
        if: github.ref == 'refs/heads/main' && github.repository == 'coiled/coiled-runtime'
        uses: JamesIves/github-pages-deploy-action@4.1.7
        with:
          branch: gh-pages
          folder: static
